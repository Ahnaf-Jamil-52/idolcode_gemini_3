# Cognitive Mirror System üß†ü™û

**Turning Practice Into Metacognition, Not Grinding**

The Cognitive Mirror is an intelligent system that transforms algorithmic problem-solving practice from mechanical repetition into meaningful learning. It answers two critical questions every time:

1. **"Why this problem, for you, now?"** ‚Äî Problem Intent Engine
2. **"What kind of thinker you behaved like?"** ‚Äî Failure Archetype Detection

## Overview

```
Cognitive Mirror = Explanation Engine + Pattern Matcher
```

This system provides:
- üéØ **Intelligent Problem Assignment** with deep pedagogical intent
- üé≠ **Failure Pattern Recognition** across 7 behavioral archetypes
- üí° **Metacognitive Feedback** that reveals thinking patterns
- üìà **Trajectory Alignment** matching your growth to champion paths

## Architecture

```
coach_engine/
‚îú‚îÄ‚îÄ cognitive_mirror.py       # Main system orchestrator
‚îú‚îÄ‚îÄ problem_intent.py         # "Why this problem?" engine
‚îú‚îÄ‚îÄ failure_archetypes.py     # "What thinker are you?" detector
‚îî‚îÄ‚îÄ ...
```

## The 7 Failure Archetypes

Every coder has failure patterns. The system detects:

| Archetype | Description | Detection Signals |
|-----------|-------------|-------------------|
| üî® **Brute Forcer** | Over-enumerates, ignores constraints | TLE, takes too long, ignores optimization |
| üìã **Pattern Chaser** | Applies memorized templates blindly | Fast fails, wrong template usage |
| ü§î **Hesitator** | Knows solution but lacks confidence | Long thinking, no submissions |
| üéØ **Overfitter** | Solves samples, fails edge cases | Multiple WA on same problem |
| üö´ **Avoider** | Subconsciously skips certain tags | Strong tag preferences/avoidance |
| ‚ö° **Speed Demon** | Rushes, makes careless mistakes | Fast submissions, many WA |
| üé® **Perfectionist** | Overthinks, times out before submitting | Very long times, analysis paralysis |

## Quick Start

### 1. Run the Demo

```bash
python main.py cognitive
```

This runs three scenarios:
- Intelligent problem assignment
- Failure pattern detection
- Pattern breaking and success

### 2. Use in Your Code

```python
from coach_engine import CognitiveMirror, ProblemMetadata, UserSkillProfile, ProblemAttempt

# Load your problem database
problems = [...]  # List of ProblemMetadata

# Initialize the mirror
mirror = CognitiveMirror(problem_database=problems)

# Create user profile
user = UserSkillProfile(
    user_id="alice",
    current_rating=1200,
    weak_skills={"dp", "greedy"},
    avoided_tags={"dp"}
)

# Start session
mirror.start_session(user.user_id, "session_001", initial_rating=1200)

# Assign a problem with explanation
problem, reflection = mirror.assign_problem(user)
print(reflection.message)  # Shows "Why this problem, for you, now?"

# After user attempts the problem
attempt = ProblemAttempt(
    problem_id=problem.problem_id,
    timestamp=datetime.now(),
    time_spent_seconds=1800,
    submission_count=3,
    final_verdict="TLE",
    tags=problem.tags,
    difficulty=problem.difficulty
)

# Analyze the attempt
reflection = mirror.analyze_attempt(user.user_id, attempt, user)
print(reflection.message)  # Shows "What kind of thinker you behaved like"

# Get archetype summary
summary = mirror.get_archetype_summary(user.user_id)
print(f"You are: {summary['archetype_name']}")
print(f"Intervention: {summary['intervention']}")
```

## Problem Metadata Structure

Each problem needs rich metadata (can be generated by LLM):

```json
{
  "problem_id": 1842,
  "title": "Binary Game",
  "difficulty": 1400,
  "tags": ["dp", "bitmask"],
  "hidden_skills": ["state compression", "transition reuse"],
  "cognitive_triggers": [
    "forces_abstraction",
    "punishes_greedy_intuition"
  ],
  "common_wrong_paths": [
    "bruteforce mask enumeration",
    "wrong dp dimension"
  ],
  "failure_archetypes_targeted": [
    "brute_forcer",
    "pattern_chaser"
  ],
  "historical_role": "tourist_phase_2_stabilization",
  "typical_solve_time_minutes": 35,
  "required_skills": ["basic_dp", "bitwise_operations"],
  "recommended_prerequisites": [455, 1234]
}
```

See `schemas/problem_metadata.json` for full schema.

## User Skill Profile

Track user strengths, weaknesses, and patterns:

```python
user_profile = UserSkillProfile(
    user_id="alice",
    
    # Skill ratings (Codeforces-style)
    skill_ratings={"dp": 1200, "greedy": 1400, "graphs": 1100},
    
    # Weak areas (need improvement)
    weak_skills={"dp", "bitmask"},
    
    # Strong areas (can handle harder)
    strong_skills={"greedy", "implementation"},
    
    # Avoidance patterns (tags user dodges)
    avoided_tags={"dp"},
    
    # Current phase
    trajectory_phase="growth",  # foundation, growth, mastery, expert
    
    # Overall rating
    current_rating=1200,
    
    # Recent history
    recent_problems_solved=[...],
    recent_problems_failed=[...]
)
```

## Problem Assignment Logic

When assigning a problem, the system:

1. **Computes target difficulty** based on strategic goal
2. **Identifies weak skills** from user profile
3. **Filters candidates** by difficulty, skill, and archetype
4. **Ranks by pedagogical value**
5. **Generates explanation** showing reasoning

### Strategic Goals

- `optimal_growth` ‚Äî Slight stretch (+50 rating)
- `break_through_plateau` ‚Äî Big stretch (+100 rating)
- `build_confidence` ‚Äî Easier (-100 rating)
- `stabilize` ‚Äî At-level practice
- `fill_gap` ‚Äî Target specific weak skill

### Reason Vector

Every assignment comes with a `ReasonVector` explaining:

```python
reason = ReasonVector(
    weak_skill_match=True,
    targeted_skill="dp",
    trajectory_alignment="active_growth_phase",
    failure_archetype_targeted="brute_forcer",
    archetype_correction_method="Forces constraint analysis",
    difficulty_gap=+50,
    difficulty_justification="at-level problem for steady growth",
    similar_champion_problem="tourist_breakthrough_problem",
    strategic_goal="optimal_growth"
)
```

## Failure Detection Algorithm

The detector analyzes patterns across recent problems (default: 20):

### Detection Process

1. **Record attempts** with timing, submission count, errors
2. **Compute statistics** on time patterns, error types, tag usage
3. **Score each archetype** against behavioral signatures
4. **Select best match** above confidence threshold (default: 0.6)
5. **Build evidence** with supporting behaviors

### Scoring Criteria (per archetype)

- **Time Pattern** (25%) ‚Äî Too fast, too slow, inconsistent
- **Submission Pattern** (25%) ‚Äî Many attempts, single fail, no submit
- **Error Pattern** (25%) ‚Äî TLE, WA, RTE frequencies
- **Tag Pattern** (25%) ‚Äî Avoidance and overuse patterns

### Example Detection

```python
detector = FailureArchetypeDetector()

# Record 5+ attempts showing pattern
for attempt in user_attempts:
    detector.record_attempt(attempt)

# Detect archetype
evidence = detector.detect_archetype()

if evidence:
    print(f"Archetype: {evidence.archetype.value}")
    print(f"Confidence: {evidence.confidence:.0%}")
    print(f"Evidence: {evidence.supporting_behaviors}")
```

## Cognitive Reflections

The system generates three types of metacognitive insights:

### 1. Problem Assignment Reflection

When assigning a problem:

```
üéØ Why I'm giving you Problem 1842 (Binary Game)

üìö Skill Gap: You need more practice with `dp`. 
This problem specifically trains that weakness.

üé≠ Behavioral Pattern: Directly addresses brute_forcer pattern

üìà Growth Challenge: This is +50 rating above your current level.
It's a at-level problem for steady growth.

üèÜ Champion Insight: This is a tourist_phase_2_stabilization.
Top competitors faced similar challenges at this stage.
```

### 2. Failure Analysis Reflection

When detecting a failure pattern:

```
üîç Pattern Detected: The Brute Forcer

What I observed:
You just behaved like The Brute Forcer.
Over-enumerates possibilities, ignores problem constraints. 
Likely to TLE.

Evidence:
  ‚Ä¢ Takes 1.8x longer than expected
  ‚Ä¢ Primarily fails with TLE, MLE
  ‚Ä¢ Overuses implementation, brute force

Why this matters:
This pattern explains why you got TLE on this problem.

What to do differently:
Force constraint analysis before coding. 
Teach complexity bounds.
```

### 3. Breakthrough Moment Reflection

When user succeeds but still shows pattern:

```
‚ú® Growth Moment

Breakthrough detected!
You solved this problem, but I noticed you're still 
showing signs of Speed Demon behavior.

Next level:
You're getting results, but breaking this pattern will 
unlock the next tier. Force slow down protocol. 
Require mental verification before submit.
```

## Integration Examples

### Flask API Endpoint

```python
@app.route('/api/assign-problem', methods=['POST'])
def assign_problem():
    user_id = request.json['user_id']
    user_profile = get_user_profile(user_id)
    
    problem, reflection = mirror.assign_problem(user_profile)
    
    return jsonify({
        'problem': problem.to_dict(),
        'explanation': reflection.message,
        'archetype': reflection.detected_archetype.value if reflection.detected_archetype else None
    })

@app.route('/api/submit-attempt', methods=['POST'])
def submit_attempt():
    data = request.json
    attempt = ProblemAttempt(**data)
    
    reflection = mirror.analyze_attempt(
        user_id=data['user_id'],
        attempt=attempt
    )
    
    if reflection:
        return jsonify({
            'feedback': reflection.message,
            'archetype': reflection.detected_archetype.value,
            'confidence': reflection.confidence
        })
    
    return jsonify({'feedback': 'Keep practicing!'})
```

### Discord Bot

```python
@bot.command()
async def practice(ctx):
    user_id = str(ctx.author.id)
    user_profile = get_profile(user_id)
    
    problem, reflection = mirror.assign_problem(user_profile)
    
    embed = discord.Embed(
        title=f"Problem {problem.problem_id}: {problem.title}",
        description=reflection.message,
        color=0x00ff00
    )
    embed.add_field(name="Difficulty", value=problem.difficulty)
    embed.add_field(name="Tags", value=", ".join(problem.tags))
    
    await ctx.send(embed=embed)
```

## Testing

Run the test suite:

```bash
pytest tests/test_cognitive_mirror.py -v
```

Tests cover:
- Archetype detection accuracy
- Problem selection logic
- Reflection generation
- Edge cases and error handling

## Configuration

### Detector Settings

```python
detector = FailureArchetypeDetector(
    lookback_problems=20  # How many recent problems to analyze
)
```

### Engine Settings

```python
engine = ProblemIntentEngine(
    problem_database=problems,
    use_gemini=True,  # Use Gemini for enhanced explanations
    gemini_api_key="your-key"
)
```

### Mirror Settings

```python
mirror = CognitiveMirror(
    problem_database=problems,
    use_gemini=True,
    gemini_api_key="your-key"
)
```

## Advanced Usage

### Custom Archetype Signatures

You can modify archetype detection by updating `ARCHETYPE_SIGNATURES`:

```python
from coach_engine.failure_archetypes import ARCHETYPE_SIGNATURES, FailureArchetype

# Customize detection thresholds
ARCHETYPE_SIGNATURES[FailureArchetype.BRUTE_FORCER].confidence_threshold = 0.7
ARCHETYPE_SIGNATURES[FailureArchetype.BRUTE_FORCER].min_problems_for_detection = 10
```

### Problem Database Generation

Use Gemini to generate metadata for existing problems:

```python
from coach_engine.gemini_analyzer import GeminiCoachAnalyzer

analyzer = GeminiCoachAnalyzer(gemini_api_key="your-key")

# Generate metadata for a problem
metadata = analyzer.generate_problem_metadata(
    problem_id=1842,
    problem_statement="...",
    difficulty=1400,
    tags=["dp", "bitmask"]
)
```

### Multi-User Sessions

Track multiple users simultaneously:

```python
mirror = CognitiveMirror(problems)

# User 1
mirror.start_session("alice", "session_alice", 1200)
problem_alice, _ = mirror.assign_problem(alice_profile)

# User 2
mirror.start_session("bob", "session_bob", 1400)
problem_bob, _ = mirror.assign_problem(bob_profile)

# Each user has independent tracking
```

## Best Practices

1. **Rich Problem Metadata** ‚Äî Better metadata = better recommendations
2. **Consistent Attempt Recording** ‚Äî Track all attempts, not just submissions
3. **Regular Profile Updates** ‚Äî Keep skill ratings current
4. **Pattern Awareness** ‚Äî Show users their detected patterns
5. **Balanced Challenge** ‚Äî Mix stretch problems with confidence builders

## Performance

- **Problem Selection**: O(n) where n = problems in difficulty range
- **Archetype Detection**: O(k) where k = lookback window (default 20)
- **Memory**: ~1KB per problem metadata, ~500B per attempt

For 10,000 problems and 100 users:
- Database: ~10MB
- Per-user state: ~50KB
- Real-time assignment: <10ms

## Limitations

- Requires 5+ attempts for archetype detection
- Problem metadata must be pre-generated
- Best with rating systems (Codeforces-style)
- Pattern detection may lag behavioral changes

## Future Enhancements

- [ ] Multi-archetype detection (blend of patterns)
- [ ] Dynamic difficulty adjustment
- [ ] Peer comparison insights
- [ ] Gemini-powered metadata generation
- [ ] Real-time coaching during solve
- [ ] Historical champion trajectory matching

## Contributing

See main project README for contribution guidelines.

## License

Part of the Coach Engine project. See root LICENSE file.

## References

- Codeforces rating system
- Educational psychology research on metacognition
- Behavioral pattern recognition in MOOCs

---

**Made with üß† for competitive programmers who want to learn, not just grind.**
